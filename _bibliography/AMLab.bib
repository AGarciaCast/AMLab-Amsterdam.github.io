---
---


@inproceedings{keller2021modeling,
title={Modeling Category-Selective Cortical Regions with Topographic Variational Autoencoders},
author={T. Anderson Keller and Qinghe Gao and Max Welling},
booktitle={SVRHM 2021 Workshop at NeurIPS},
year={2021},
pdf={https://arxiv.org/pdf/2110.13911.pdf},
abstract = {Category-selectivity in the brain describes the observation that certain spatially localized areas of the cerebral cortex tend to respond robustly and selectively to stimuli from specific limited categories. One of the most well known examples of category-selectivity is the Fusiform Face Area (FFA), an area of the inferior temporal cortex in primates which responds preferentially to images of faces when compared with objects or other generic stimuli. In this work, we leverage the newly introduced Topographic Variational Autoencoder to model the emergence of such localized category-selectivity in an unsupervised manner. Experimentally, we demonstrate our model yields spatially dense neural clusters selective to faces, bodies, and places through visualized maps of Cohen's d metric. We compare our model with related supervised approaches, namely the Topographic Deep Artificial Neural Network (TDANN) of Lee et al., and discuss both theoretical and empirical similarities. Finally, we show preliminary results suggesting that our model yields a nested spatial hierarchy of increasingly abstract categories, analogous to observations from the human ventral temporal cortex.},
abbr={NeurIPS}
}

@InProceedings{Keller_2021_ICCV,
    author    = {Keller, T. Anderson and Welling, Max},
    title     = {Predictive Coding With Topographic Variational Autoencoders},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2021},
    pages     = {1086-1091},
    pdf = {https://openaccess.thecvf.com/content/ICCV2021W/VIPriors/papers/Keller_Predictive_Coding_With_Topographic_Variational_Autoencoders_ICCVW_2021_paper.pdf},
    abstract = {Category-selectivity in the brain describes the observation that certain spatially localized areas of the cerebral cortex tend to respond robustly and selectively to stimuli from specific limited categories. One of the most well known examples of category-selectivity is the Fusiform Face Area (FFA), an area of the inferior temporal cortex in primates which responds preferentially to images of faces when compared with objects or other generic stimuli. In this work, we leverage the newly introduced Topographic Variational Autoencoder to model of the emergence of such localized category-selectivity in an unsupervised manner. Experimentally, we demonstrate our model yields spatially dense neural clusters selective to faces, bodies, and places through visualized maps of Cohen's d metric. We compare our model with related supervised approaches, namely the TDANN, and discuss both theoretical and empirical similarities. Finally, we show preliminary results suggesting that our model yields a nested spatial hierarchy of increasingly abstract categories, analogous to observations from the human ventral temporal cortex.},
    abbr = {ICCV}
}

@inproceedings{NEURIPS2021_f03704cb,
 author = {Keller, T. Anderson and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28585--28597},
 publisher = {Curran Associates, Inc.},
 title = {Topographic VAEs learn Equivariant Capsules},
 pdf = {https://proceedings.neurips.cc/paper/2021/file/f03704cb51f02f80b09bffba15751691-Paper.pdf},
 volume = {34},
 year = {2021},
 abstract = {In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. "capsules") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.},
 abbr = {NeurIPS}
}

@InProceedings{pmlr-v139-keller21a,
  title =    {Self Normalizing Flows},
  author =       {Keller, T. Anderson and Peters, Jorn W.T. and Jaini, Priyank and Hoogeboom, Emiel and Forr{\'e}, Patrick and Welling, Max},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning},
  pages =    {5378--5387},
  year =     {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v139/keller21a/keller21a.pdf},
  url =      {https://proceedings.mlr.press/v139/keller21a.html},
  abstract =     {Efficient gradient computation of the Jacobian determinant term is a core problem in many machine learning settings, and especially so in the normalizing flow framework. Most proposed flow models therefore either restrict to a function class with easy evaluation of the Jacobian determinant, or an efficient estimator thereof. However, these restrictions limit the performance of such density models, frequently requiring significant depth to reach desired performance levels. In this work, we propose \emph{Self Normalizing Flows}, a flexible framework for training normalizing flows by replacing expensive terms in the gradient by learned approximate inverses at each layer. This reduces the computational complexity of each layerâ€™s exact update from $\mathcal{O}(D^3)$ to $\mathcal{O}(D^2)$, allowing for the training of flow architectures which were otherwise computationally infeasible, while also providing efficient sampling. We show experimentally that such models are remarkably stable and optimize to similar data likelihood values as their exact gradient counterparts, while training more quickly and surpassing the performance of functionally constrained counterparts.},
  abbr = {ICML}
}

@InProceedings{DBLP:journals/corr/abs-2106-15577,
  author    = {Fiorella Wever and
               T. Anderson Keller and
               Victor Garcia and
               Laura Symul},
  title     = {As easy as {APC:} Leveraging self-supervised learning in the context
               of time series classification with varying levels of sparsity and
               severe class imbalance},
  booktitle   = {Self-Supervised Learning Workshop at NeurIPS},
  year      = {2021},
  pdf       = {https://arxiv.org/pdf/2106.15577.pdf},
  abbr = {NeurIPS},
  abstract = {High levels of missing data and strong class imbalance are ubiquitous challenges that are often presented simultaneously in real-world time series data. Existing methods approach these problems separately, frequently making significant assumptions about the underlying data generation process in order to lessen the impact of missing information. In this work, we instead demonstrate how a general self-supervised training method, namely Autoregressive Predictive Coding (APC), can be leveraged to overcome both missing data and class imbalance simultaneously without strong assumptions. Specifically, on a synthetic dataset, we show that standard baselines are substantially improved upon through the use of APC, yielding the greatest gains in the combined setting of high missingness and severe class imbalance. We further apply APC on two real-world medical time-series datasets, and show that APC improves the classification performance in all settings, ultimately achieving state-of-the-art AUPRC results on the Physionet benchmark}
}


@inproceedings{moretti2021variational,
  title={Variational combinatorial sequential {M}onte {C}arlo methods for {B}ayesian phylogenetic inference},
  author={Moretti, Antonio Khalil and Zhang, Liyi and Naesseth, Christian A. and Venner, Hadiah and Blei, David and Pe'er, Itsik},
  booktitle={Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  abbr={UAI},
  pages={971--981},
  year={2021},
  editor={de Campos, Cassio and Maathuis, Marloes H.},
  volume={161},
  series={Proceedings of Machine Learning Research},
  month={27--30 Jul},
  publisher={PMLR},
  pdf={https://proceedings.mlr.press/v161/moretti21a/moretti21a.pdf},
  url={https://proceedings.mlr.press/v161/moretti21a.html},
 }



@article{lowe2022amortized,
  title={{Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data}},
  author={S. L{\"o}we and D. Madras and R. Zemel and M. Welling},
  journal={Causal Learning and Reasoning},
  year={2022},
  html = {https://arxiv.org/abs/2006.10833},
  pdf = {https://arxiv.org/pdf/2006.10833.pdf},
  abbr={CLeaR},
  code = {https://github.com/loeweX/AmortizedCausalDiscovery},
  abstract={On time-series data, most causal discovery methods fit a new model whenever they encounter samples from a new underlying causal graph. However, these samples often share relevant information which is lost when following this approach. Specifically, different samples may share the dynamics which describe the effects of their causal relations. We propose Amortized Causal Discovery, a novel framework that leverages such shared dynamics to learn to infer causal relations from time-series data. This enables us to train a single, amortized model that infers causal relations across samples with different underlying causal graphs, and thus leverages the shared dynamics information. We demonstrate experimentally that this approach, implemented as a variational model, leads to significant improvements in causal discovery performance, and show how it can be extended to perform well under added noise and hidden confounding.}
}



@inproceedings{brandstetter2022geometric,
  title={Geometric and Physical Quantities improve E (3) Equivariant Message Passing},
  author={Brandstetter, Johannes and Hesselink, Rob and van der Pol, Elise and Bekkers, Erik and Welling, Max},
  booktitle={International Conference on Learning Representations},
  abbr={ICLR},
  year={2022},
  abstract={Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(3) Equivariant Graph Neural Networks (SEGNNs) that generalise equivariant graph networks, such that node and edge attributes are not restricted to invariant scalars, but can contain covariant information, such as vectors or tensors. This model, composed of steerable MLPs, is able to incorporate geometric and physical information in both the message and update functions. Through the definition of steerable node attributes, the MLPs provide a new class of activation functions for general use with steerable feature fields. We discuss ours and related work through the lens of equivariant non-linear convolutions, which further allows us to pin-point the successful components of SEGNNs: non-linear message aggregation improves upon classic linear (steerable) point convolutions; steerable messages improve upon recent equivariant graph networks that send invariant messages. We demonstrate the effectiveness of our method on several tasks in computational physics and chemistry and provide extensive ablation studies.},
  html={https://openreview.net/forum?id=_xwr8gOBeV1},
  pdf={https://arxiv.org/pdf/2110.02905},
  code={https://github.com/RobDHess/Steerable-E3-GNN},
}


@inproceedings{ruhe2022selfsupervised,
  title={Self-Supervised Inference in State-Space Models},
  author={David Ruhe and Patrick Forr{\'e}},
  booktitle={International Conference on Learning Representations},
  abbr={ICLR},
  year={2022},
  abstract={We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. It comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.},
  html={https://openreview.net/forum?id=VPjw9KPWRSK},
  pdf={https://openreview.net/pdf?id=VPjw9KPWRSK},
}

@inproceedings{bozkurt2021rateregularization,
  title = {Rate-{{Regularization}} and {{Generalization}} in {{Variational Autoencoders}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Bozkurt, Alican and Esmaeili, Babak and Tristan, Jean-Baptiste and Brooks, Dana and Dy, Jennifer and van de Meent, Jan-Willem},
  year = {2021},
  month = mar,
  pages = {3880--3888},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Variational autoencoders (VAEs) optimize an objective that comprises a reconstruction loss (the distortion) and a KL term (the rate). The rate is an upper bound on the mutual information, which is...},
  langid = {english},
  abbr = {AISTATS},
  html = {https://proceedings.mlr.press/v130/bozkurt21a.html},
  pdf = {http://proceedings.mlr.press/v130/bozkurt21a/bozkurt21a.pdf}
}

@inproceedings{zimmermann2021nested,
  title={Nested Variational Inference},
  author={Heiko Zimmermann and Hao Wu and Babak Esmaeili and Jan-Willem van de Meent},
  booktitle = {Advances in Neural Information Processing Systems},
  year={2021},
  volume={34},
  abbr={NeurIPS},
  html={https://proceedings.neurips.cc/paper/2021/hash/ab49b208848abe14418090d95df0d590-Abstract.html},
  pdf={https://proceedings.neurips.cc/paper/2021/file/ab49b208848abe14418090d95df0d590-Paper.pdf},
}


@InProceedings{wu2021conjugate,
  title =    {Conjugate Energy-Based Models},
  author =     {Wu, Hao* and Esmaeili, Babak* and Wick, Michael and Tristan, Jean-Baptiste and {van de Meent}, Jan-Willem},
  booktitle =    {Proceedings of the 38th International Conference on Machine Learning ({{ICML}})},
  pages =    {11228--11239},
  year =   {2021},
  editor =   {Meila, Marina and Zhang, Tong},
  volume =   {139},
  series =   {Proceedings of Machine Learning Research},
  month =    {18--24 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v139/wu21a/wu21a.pdf},
  html =    {https://proceedings.mlr.press/v139/wu21a.html},
  abstract =   {In this paper, we propose conjugate energy-based models (CEBMs), a new class of energy-based models that define a joint density over data and latent variables. The joint density of a CEBM decomposes into an intractable distribution over data and a tractable posterior over latent variables. CEBMs have similar use cases as variational autoencoders, in the sense that they learn an unsupervised mapping from data to latent variables. However, these models omit a generator network, which allows them to learn more flexible notions of similarity between data points. Our experiments demonstrate that conjugate EBMs achieve competitive results in terms of image modelling, predictive power of latent space, and out-of-domain detection on a variety of datasets.},
  abbr = {ICML}
}


@inproceedings{stites-zimmerman2021LearningProposals,
  title       = {Learning proposals for probabilistic programs with inference combinators},
  author      = {Stites, Sam and Zimmermann, Heiko and Wu, Hao and Sennesh, Eli and van de Meent, Jan-Willem},
  shortauthor = {Stites and Zimmermann},
  booktitle   = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages       = {1056--1066},
  year        = {2021},
  editor      = {de Campos, Cassio and Maathuis, Marloes H.},
  volume      = {161},
  series      = {Proceedings of Machine Learning Research},
  month       = {27--30 Jul},
  publisher   = {PMLR},
  pdf         = {https://proceedings.mlr.press/v161/stites21a/stites21a.pdf},
  html        = {https://proceedings.mlr.press/v161/stites21a.html},
  abstract    = {We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing.},
  code = {https://github.com/probtorch/combinators/},
  abbr = {UAI}
}


@inproceedings{bekkers2019b,
  title={B-Spline CNNs on Lie groups},
  author={Bekkers, Erik J},
  booktitle={International Conference on Learning Representations},
  abbr={ICLR},
  year={2019},
  abstract={Group convolutional neural networks (G-CNNs) can be used to improve classical CNNs by equipping them with the geometric structure of groups. Central in the success of G-CNNs is the lifting of feature maps to higher dimensional disentangled representations, in which data characteristics are effectively learned, geometric data-augmentations are made obsolete, and predictable behavior under geometric transformations (equivariance) is guaranteed via group theory. Currently, however, the practical implementations of G-CNNs are limited to either discrete groups (that leave the grid intact) or continuous compact groups such as rotations (that enable the use of Fourier theory). In this paper we lift these limitations and propose a modular framework for the design and implementation of G-CNNs for arbitrary Lie groups. In our approach the differential structure of Lie groups is used to expand convolution kernels in a generic basis of B-splines that is defined on the Lie algebra. This leads to a flexible framework that enables localized, atrous, and deformable convolutions in G-CNNs by means of respectively localized, sparse and non-uniform B-spline expansions. The impact and potential of our approach is studied on two benchmark datasets: cancer detection in histopathology slides (PCam dataset) in which rotation equivariance plays a key role and facial landmark localization (CelebA dataset) in which scale equivariance is important. In both cases, G-CNN architectures outperform their classical 2D counterparts and the added value of atrous and localized group convolutions is studied in detail.},
  html={https://openreview.net/forum?id=H1gBhkBFDH},
  pdf={https://openreview.net/pdf?id=H1gBhkBFDH},
  video={https://www.youtube.com/watch?v=rakcnrgX4oo},
  code={https://github.com/ebekkers/gsplinets},
  selected=true
}


@article{esmaeili2018structured,
  title = {Structured {{Disentangled Representations}}},
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  journal = {Artificial Intelligence and Statistics},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and {van de Meent}, Jan-Willem},
  year = {2019},
  abbr = {AISTATS},
  pdf = {https://proceedings.mlr.press/v89/esmaeili19a/esmaeili19a.pdf},
  html = {https://proceedings.mlr.press/v89/esmaeili19a},
  selected = true
}

@article{vandemeent2018introduction,
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  abbr = {arXiv},
  journal = {arXiv:1809.10756 [cs, stat]},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and   Yang, Hongseok and Wood, Frank},
  month = sep,
  year = {2018},
  pdf = {https://arxiv.org/pdf/1809.10756},
  html = {https://arxiv.org/abs/1809.10756},
  selected=false
}

@inproceedings{siddharth_nips_2017,
    title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
    abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
    author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood,
Frank and Torr, Philip},
    booktitle = {Advances in Neural Information Processing Systems 30},
    editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    pages = {5927--5937},
    year = {2017},
    abbr = {NeurIPS},
    html = {https://proceedings.neurips.cc/paper/2017/hash/9cb9ed4f35cf7c2f295cc2bc6f732a84-Abstract.html},
    pdf = {https://proceedings.neurips.cc/paper/2017/file/9cb9ed4f35cf7c2f295cc2bc6f732a84-Paper.pdf},
    code = {https://github.com/probtorch/probtorch},
    selected = true
}

@InProceedings{pmlr-v130-nalisnick21a,
  title = {Predictive Complexity Priors},
  author = {Nalisnick, Eric and Gordon, Jonathan and Miguel Hernandez-Lobato, Jose},
  booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = {694--702},
  year = {2021},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  volume = {130},
  series = {Proceedings of Machine Learning Research},
  month = {13--15 Apr},
  publisher = {PMLR},
  abbr = {AISTATS},
  html = {https://proceedings.mlr.press/v130/nalisnick21a.html},
  pdf = {http://proceedings.mlr.press/v130/nalisnick21a/nalisnick21a.pdf},
}

@inproceedings{nalisnick2018do,
  title={Do Deep Generative Models Know What They Don't Know?},
  author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
  booktitle={International Conference on Learning Representations},
  year={2019},
  abbr = {ICLR},
  html = {https://openreview.net/forum?id=H1xwNhCcYm},
  pdf = {https://openreview.net/pdf?id=H1xwNhCcYm},
  selected = true
}


@InProceedings{pmlr-v139-daxberger21a,
  title = 	 {Bayesian Deep Learning via Subnetwork Inference},
  author =       {Daxberger, Erik and Nalisnick, Eric and Allingham, James U and Antoran, Javier and Hernandez-Lobato, Jose Miguel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2510--2521},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/daxberger21a/daxberger21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/daxberger21a.html},
  abbr = {ICML}
}

@article{JMLR:v22:19-1028,
  author  = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
  title   = {Normalizing Flows for Probabilistic Modeling and Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {57},
  pages   = {1-64},
  abbr = {JMLR},
  url     = {http://jmlr.org/papers/v22/19-1028.html},
  pdf = 	 {https://jmlr.org/papers/volume22/19-1028/19-1028.pdf}
}

@inproceedings{schlichtkrull2018modeling,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van~den Berg, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, RaphaÃ«l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  options = {useprefix=true},
  year = {2018},
  pages = {593--607},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  html = {https://link.springer.com/chapter/10.1007/978-3-319-93417-4_38},
  pdf = {https://arxiv.org/pdf/1703.06103.pdf},
  abbr = {ESWC},
  selected = true
}


@InProceedings{pmlr-v48-cohenc16,
  title =    {Group Equivariant Convolutional Networks},
  author =   {Cohen, Taco and Welling, Max},
  booktitle =    {Proceedings of The 33rd International Conference on Machine Learning},
  pages =    {2990--2999},
  year =   {2016},
  editor =   {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume =   {48},
  series =   {Proceedings of Machine Learning Research},
  address =    {New York, New York, USA},
  month =    {20--22 Jun},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v48/cohenc16.pdf},
  url =    {https://proceedings.mlr.press/v48/cohenc16.html},
  abstract =   {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  html = {https://proceedings.mlr.press/v48/cohenc16.html},
  pdf = {http://proceedings.mlr.press/v48/cohenc16.pdf},
  abbr = {ICML},
  selected = true
}

@inproceedings{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={International Conference on Learning Representations},
  abbr = {ICLR},
  year= {2017},
  html = {https://openreview.net/forum?id=SJU4ayYgl},
  pdf = {https://arxiv.org/pdf/1609.02907.pdf},
  selected = true,
}

@inproceedings{NIPS2016_ddeebdee,
 author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Improved Variational Inference with Inverse Autoregressive Flow},
 abbr = {NeurIPS},
 html = {https://proceedings.neurips.cc/paper/2016/hash/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Abstract.html},
 pdf = {https://proceedings.neurips.cc/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
 volume = {29},
 year = {2016},
 selected = true
}


@inproceedings{NIPS2014_d523773c,
  title = {Semi-Supervised Learning with Deep Generative Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. and Weinberger, K. Q.},
  year = {2014},
  volume = {27},
  abbr = {NeurIPS},
  publisher = {{Curran Associates, Inc.}},
  html = {https://proceedings.neurips.cc/paper/2014/hash/d523773c6b194f37b938d340d5d02232-Abstract.html},
  pdf = {https://proceedings.neurips.cc/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
  selected = true
}

@article{kingma2013autoencoding,
  title = {Auto-Encoding Variational Bayes},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  journaltitle = {International Conference on Learning Representations},
  abbr={ICLR},
  html={https://openreview.net/forum?id=33X9fd2-9FyZd},
  pdf={https://arxiv.org/pdf/1312.6114v10.pdf},
  selected= true
}

@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient langevin dynamics},
  author={Welling, Max and Teh, Yee Whye},
  booktitle={Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  html = {http://www.icml-2011.org/papers.php},
  pdf = {http://www.icml-2011.org/papers/398_icmlpaper.pdf},
  video={https://icml.cc/virtual/2021/test-of-time/11808},
  abbr = {ICML},
  year = {2011},
  selected = true
}

@inproceedings{pol2020mdp,
title = {MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning},
author = {van der Pol, Elise and Worrall, Daniel and van Hoof, Herke and Oliehoek, Frans and Welling, Max},
url = {https://arxiv.org/abs/2006.16908
https://proceedings.neurips.cc/paper/2020/hash/2be5f9c2e3620eb73c2972d7552b6cb5-Abstract.html},
year = {2020},
date = {02-12-2020},
booktitle = {Advances in Neural Information Processing Systems},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
selected=true,
html={https://proceedings.neurips.cc/paper/2020/hash/2be5f9c2e3620eb73c2972d7552b6cb5-Abstract.html},
pdf={https://proceedings.neurips.cc/paper/2020/file/2be5f9c2e3620eb73c2972d7552b6cb5-Paper.pdf},
abbr={NeurIPS}
}


@inproceedings{pol2022multi,
title = {Multi-Agent MDP Homomorphic Networks},
author = {van der Pol, Elise and van Hoof, Herke and Oliehoek, Frans and Welling, Max},
year = {2022},
date = {25-04-2022},
booktitle = {International Conference on Learning Representations},
keywords = {},
pubstate = {forthcoming},
abbr={ICLR},
abstract={This paper introduces Multi-Agent MDP Homomorphic Networks, a class of networks that allows distributed execution using only local information, yet is able to share experience between global symmetries in the joint state-action space of cooperative multi-agent systems. In cooperative multi-agent systems, complex symmetries arise between different configurations of the agents and their local observations. For example, consider a group of agents navigating: rotating the state globally results in a permutation of the optimal joint policy. Existing work on symmetries in single agent reinforcement learning can only be generalized to the fully centralized setting, because such approaches rely on the global symmetry in the full state-action spaces, and these can result in correspondences across agents. To encode such symmetries while still allowing distributed execution we propose a factorization that decomposes global symmetries into local transformations. Our proposed factorization allows for distributing the computation that enforces global symmetries over local agents and local interactions. We introduce a multi-agent equivariant policy network based on this factorization. We show empirically on symmetric multi-agent problems that globally symmetric distributable policies improve data efficiency compared to non-equivariant baselines.},
selected=false,
html={https://openreview.net/forum?id=H7HDG\hyphen\hyphenDJF0},
pdf={https://openreview.net/pdf?id=H7HDG\hyphen\hyphenDJF0}
}

@inproceedings{kool2020estimating,
title = {Estimating Gradients for Discrete Random Variables by Sampling without Replacement},
author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
url = {https://openreview.net/pdf?id=rklEj2EFvB
https://youtu.be/KtP-Z2bvPPE},
year = {2020},
date = {2020-04-26},
booktitle = {International Conference on Learning Representations},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
abbr={ICLR},
abstract={We derive an unbiased estimator for expectations over discrete random variables based on sampling without replacement, which reduces variance as it avoids duplicate samples. We show that our estimator can be derived as the Rao-Blackwellization of three different estimators. Combining our estimator with REINFORCE, we obtain a policy gradient estimator and we reduce its variance using a built-in control variate which is obtained without additional model evaluations. The resulting estimator is closely related to other gradient estimators. Experiments with a toy problem, a categorical Variational Auto-Encoder and a structured prediction problem show that our estimator is the only estimator that is consistently among the best estimators in both high and low entropy settings.},
video={https://youtu.be/KtP-Z2bvPPE},
code={https://github.com/wouterkool/estimating-gradients-without-replacement},
selected=true,
html={https://openreview.net/forum?id=rklEj2EFvB},
pdf={https://openreview.net/pdf?id=rklEj2EFvB},
}

@article{wang2021optimizing,
title = {Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator},
author = {Wang, Shihan and Zhang, Chao and KrÃ¶se, Ben and van Hoof, Herke},
url = {https://link.springer.com/article/10.1007/s10916-021-01773-0},
doi = {https://doi.org/10.1007/s10916-021-01773-0},
year = {2021},
date = {18-10-2021},
journal = {Journal of Medical Systems},
volume = {45},
number = {102},
keywords = {},
pubstate = {published},
tppubtype = {article},
html={https://link.springer.com/article/10.1007/s10916-021-01773-0a},
abbr={JMS}
}

@article{wang2021reinforcement,
title = {Reinforcement Learning to Send Reminders at Right Moments in Smartphone Exercise Application: A Feasibility Study},
author = {S. Wang and K. Sporrel and H. van Hoof and M. Simons and R. de Boer and D. Ettema and N. Nibbeling and M. Deutekom and B. KrÃ¶se},
url = {https://www.mdpi.com/1660-4601/18/11/6059
https://www.mdpi.com/1660-4601/18/11/6059/pdf},
year = {2021},
date = {2021-04-06},
journal = {International Journal of Environmental Research and Public Health, Special Issue},
keywords = {},
pubstate = {published},
tppubtype = {article},
html={https://www.mdpi.com/1660-4601/18/11/6059},
pdf={https://www.mdpi.com/1660-4601/18/11/6059/pdf},
abbr={IJERPH}
}

@inproceedings{woehlke2021hierarchies,
title = {Hierarchies of Planning and Reinforcement Learning for Robot Navigation},
author = {J. WÃ¶hlke and F. Schmitt and van Hoof, H. },
url = {https://arxiv.org/abs/2109.11178},
year = {2021},
date = {2021-05-30},
booktitle = {IEEE International Conference on Robotics and Automation},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
code={https://github.com/boschresearch/Hierarchies-of-Planning-and-Reinforcement-Learning-for-Robot-Navigation},
html={https://ieeexplore.ieee.org/abstract/document/9561151},
pdf={https://arxiv.org/pdf/2109.11178.pdf},
abbr={ICRA}
}

@inproceedings{zhang2021deep,
title = {Deep Coherent Exploration For Continuous Control},
author = {Zhang, Yijie and van Hoof, Herke },
url = {http://proceedings.mlr.press/v139/zhang21t.html},
year = {2021},
date = {2021-07-19},
booktitle = {International Conference on Machine Learning},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings},
html={http://proceedings.mlr.press/v139/zhang21t.html},
pdf={http://proceedings.mlr.press/v139/zhang21t/zhang21t.pdf},
abbr={ICML}
}

@inproceedings{kool2022deep,
title = {Deep Policy Dynamic Programming for Vehicle Routing Problems},
author = {Kool, Wouter and van Hoof, Herke and Gromicho, Joaquim and Welling, Max},
url = {https://arxiv.org/abs/2102.11756},
year = {2022},
date = {20-06-2022},
booktitle = {International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
code={https://github.com/wouterkool/dpdp},
pdf={https://arxiv.org/pdf/2102.11756},
abbr={CPAIOR}
}

@inproceedings{long2022fast,
title = {Fast and Data Efficient Reinforcement Learning from Pixels via Non-Parametric Value Approximation},
author = {Long, Alex and Blair, Alan and van Hoof, Herke},
url = {https://arxiv.org/abs/2203.03078},
year = {2022},
date = {2022-02-21},
booktitle = {AAAI National Conference on Artificial Intelligence},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={},
pdf={https://www.aaai.org/AAAI22Papers/AAAI-8450.LongA.pdf},
abbr={AAAI}
}

@inproceedings{wohlke2022value,
title = {Value Refinement Network (VRN)},
author = {Jan WÃ¶hlke and Felix Schmitt and Herke van Hoof},
url = {https://openreview.net/forum?id=iUt2KYdXBDD},
year = {2022},
date = {23-7-2022},
booktitle = {International Joint Conference on Artificial Intelligence},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={https://openreview.net/forum?id=iUt2KYdXBDD},
abbr={IJCAI}
}

@inproceedings{hopner2022leveraging,
title = {Leveraging class abstraction for commonsense reinforcement learning via residual policy gradient methods},
author = {HÃ¶pner, Niklas and Tiddi, Ilaria and van Hoof, Herke},
url = {https://arxiv.org/abs/2201.12126},
year = {2022},
date = {23-7-2022},
booktitle = {International Joint Conference on Artificial Intelligence},
keywords = {},
pubstate = {forthcoming},
tppubtype = {inproceedings},
html={https://arxiv.org/abs/2201.12126},
pdf={https://arxiv.org/pdf/2201.12126.pdf},
abbr={IJCAI}
}

@inproceedings{bakker2020single,
  author={T. Bakker and H. van Hoof and Max Welling},
  title={Experimental design for {MRI} by greedy policy search},
  month={Dec},
  year={2020},
  booktitle={Advances in Neural Information Processing Systems 33},
  abbr={NeurIPS},
  pdf={https://arxiv.org/pdf/2010.16262.pdf},
  code={https://github.com/Timsey/pg_mri},
  note={Spotlight presentation},
}

@article{kanis2021btb,
  title={{Back to Basics: Deep Reinforcement Learning in Traffic Signal Control}},
  author={S. Kanis and L. Samson and D. Bloembergen and T. Bakker},
  journal={The 10th International Workshop on Urban Computing},
  month={Nov},
  year={2021},
  abbr={UrbComp},
  pdf={https://arxiv.org/pdf/2109.07180.pdf},
  code={https://github.com/Amsterdam-Internships/Self-Learning-Traffic-Lights},
  note={Best paper award runner-up},
}

@inproceedings{bakker2022multi,
  author={T. Bakker and M. Muckley and A. Romero-Soriano and M. Drozdzal and L. Pineda},
  title={On learning adaptive acquisition policies for undersampled multi-coil {MRI} reconstruction},
  month={July},
  year={2022},
  booktitle={Proceedings of Machine Learning Research},
  abbr={MIDL},
  pdf={https://arxiv.org/pdf/2203.16392.pdf},
  code={https://github.com/facebookresearch/fastMRI},
  note={FAIR internship, to appear},
}