@inproceedings{wang2020doubly,
  title={Doubly stochastic variational inference for neural processes with hierarchical latent variables},
  abstract = {Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce predictive distributions. However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while target specific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification.},
  author={Wang, Qi and Van Hoof, Herke},
  booktitle={International Conference on Machine Learning},
  pages={10018--10028},
  year={2020},
  organization={PMLR},
  pdf =   {http://proceedings.mlr.press/v119/wang20s/wang20s.pdf},
  abbr={ICML},
}


@inproceedings{wang2022model,
  title={Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models and Amortized Policy Search},
  abstract = {Reinforcement learning is a promising paradigm for solving sequential decision-making problems, but low data efficiency and weak generalization across tasks are bottlenecks in real-world applications. Model-based meta reinforcement learning addresses these issues by learning dynamics and leveraging knowledge from prior experience. In this paper, we take a closer look at this framework and propose a new posterior sampling based approach that consists of a new model to identify task dynamics together with an amortized policy optimization step. We show that our model, called a graph structured surrogate model (GSSM), achieves competitive dynamics prediction performance with lower model complexity. Moreover, our approach in policy search is able to obtain high returns and allows fast execution by avoiding test-time policy gradient updates.},
  author={Wang, Qi and Van Hoof, Herke},
  booktitle={International Conference on Machine Learning},
  pages={23055--23077},
  year={2022},
  organization={PMLR},
  pdf =   {https://proceedings.mlr.press/v162/wang22z/wang22z.pdf},
  abbr={ICML},
}


@InProceedings{wanglearning,
   title = {Learning Expressive Meta-Representations with Mixture of Expert Neural Processes},
   abstract = {Neural processes (NPs) formulate exchangeable stochastic processes and are promising models for meta learning that do not require gradient updates during the testing phase. However, most NP variants place a strong emphasis on a global latent variable. This weakens the approximation power and restricts the scope of applications using NP variants, especially when data generative processes are complicated. To resolve these issues, we propose to combine the Mixture of Expert models with Neural Processes to develop more expressive exchangeable stochastic processes, referred to as Mixture of Expert Neural Processes (MoE-NPs). Then we apply MoE-NPs to both few-shot supervised learning and meta reinforcement
learning tasks. Empirical results demonstrate MoE-NPsâ€™ strong generalization
capability to unseen tasks in these benchmarks.},
   author = {Wang, Qi and Van Hoof, Herke},
   booktitle = {36th Conference on Neural Information Processing Systems (NeurIPS)},
   year = {2022},
   month = {11},
   pdf = {https://drive.google.com/file/d/1LfR43Cr8TliZcwqErhT7nsAOK296BZ0g/view},
   abbr={NeurIPS},
}


