---
---

@InProceedings{pmlr-v130-nalisnick21a,
  title =    { Predictive Complexity Priors },
  author =       {Nalisnick, Eric and Gordon, Jonathan and Miguel Hernandez-Lobato, Jose},
  booktitle =    {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages =    {694--702},
  year =     {2021},
  editor =   {Banerjee, Arindam and Fukumizu, Kenji},
  volume =   {130},
  series =   {Proceedings of Machine Learning Research},
  month =    {13--15 Apr},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v130/nalisnick21a/nalisnick21a.pdf},
  url =      {https://proceedings.mlr.press/v130/nalisnick21a.html},
  abstract =     { Specifying a Bayesian prior is notoriously difficult for complex models such as neural networks. Reasoning about parameters is made challenging by the high-dimensionality and over-parameterization of the space. Priors that seem benign and uninformative can have unintuitive and detrimental effects on a model’s predictions. For this reason, we propose predictive complexity priors: a functional prior that is defined by comparing the model’s predictions to those of a reference model. Although originally defined on the model outputs, we transfer the prior to the model parameters via a change of variables. The traditional Bayesian workflow can then proceed as usual. We apply our predictive complexity prior to high-dimensional regression, reasoning over neural network depth, and sharing of statistical strength for few-shot learning. },
  selected=true
}

@inproceedings{
nalisnick2018do,
title={Do Deep Generative Models Know What They Don't Know? },
author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1xwNhCcYm},
abstract = { A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood. },
selected=true
}