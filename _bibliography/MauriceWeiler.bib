
@InProceedings{Weiler2018SFCNN,
  title     = {Learning Steerable Filters for Rotation Equivariant {CNNs}},
  author    = {Weiler, Maurice and Hamprecht, Fred A. and Storath, Martin},
  booktitle = {{Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year      =  2018,
  html      = {https://arxiv.org/abs/1711.07289},
  pdf       = {https://arxiv.org/pdf/1711.07289.pdf},
  abbr      = {{CVPR}},
  abstract  = {In many machine learning tasks it is desirable that a model’s prediction transforms in an equivariant way under transformations of its input. Convolutional neural networks (CNNs) implement translational equivariance by construction; for other transformations, however, they are compelled to learn the proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs) which achieve joint equivariance under translations and rotations by design. The proposed architecture employs steerable filters to efficiently compute orientation dependent responses for many orientations without suffering interpolation artifacts from filter rotation. We utilize group convolutions which guarantee an equivariant mapping. In addition, we generalize He’s weight initialization scheme to filters which are defined as a linear combination of a system of atomic filters. Numerical experiments show a substantial enhancement of the sample complexity with a growing number of sampled filter orientations and confirm that the network generalizes learned patterns over orientations. The proposed approach achieves state-of-the-art on the rotated MNIST benchmark and on the ISBI 2012 2D EM segmentation challenge.},
}


@InProceedings{Weiler2018SE3NCNNs,
  title     = {{3D} Steerable {CNN}s: Learning Rotationally Equivariant Features in Volumetric Data},
  author    = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S.},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year      =  2018,
  html      = {https://arxiv.org/abs/1807.02547},
  pdf       = {https://arxiv.org/pdf/1807.02547.pdf},
  abbr      = {{NeurIPS}},
  abstract  = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R^3 . Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
}


@InProceedings{Cohen2018Intertwiners,
  title     = {Intertwiners between Induced Representations (with Applications to the Theory of Equivariant Neural Networks)},
  author    = {Cohen, Taco S. and Geiger, Mario and Weiler, Maurice},
  booktitle = {arXiv preprint arXiv:1803.10743},
  year      = 2018,
  html      = {https://arxiv.org/abs/1803.10743},
  pdf       = {https://arxiv.org/pdf/1803.10743.pdf},
  abbr      = {{arXiv}},
  abstract  = {Group equivariant and steerable convolutional neural networks (regular and steerable G-CNNs) have recently emerged as a very effective model class for learning from signal data such as 2D and 3D images, video, and other data where symmetries are present. In geometrical terms, regular G-CNNs represent data in terms of scalar fields (“feature channels”), whereas the steerable G-CNN can also use vector or tensor fields (“capsules”) to represent data. In algebraic terms, the feature spaces in regular G-CNNs transform according to a regular representation of the group G, whereas the feature spaces in Steerable G-CNNs transform according to the more general induced representations of G. In order to make the network equivariant, each layer in a G-CNN is required to intertwine between the induced representations associated with its input and output space. In this paper we present a general mathematical framework for G-CNNs on homogeneous spaces like Euclidean space or the sphere. We show, using elementary methods, that the layers of an equivariant network are convolutional if and only if the input and output feature spaces transform according to an induced representation. This result, which follows from G.W. Mackey’s abstract theory on induced representations, establishes G-CNNs as a universal class of equivariant network architectures, and generalizes the important recent work of Kondor & Trivedi on the intertwiners between regular representations. In order for a convolution layer to be equivariant, the filter kernel needs to satisfy certain linear equivariance constraints. The space of equivariant kernels has a rich and interesting structure, which we expose using direct calculations. Additionally, we show how this general understanding can be used to compute a basis for the space of equivariant filter kernels, thereby providing a straightforward path to the implementation of G-CNNs for a wide range of groups and manifolds.},
}


@InProceedings{Falorsi2018homeomorphicVAEs,
  title     = {Explorations in homeomorphic variational auto-encoding},
  author    = {Falorsi, Luca and de Haan, Pim and Davidson, Tim R and De Cao, Nicola and Weiler, Maurice and Forr{\'e}, Patrick and Cohen, Taco S},
  booktitle = {arXiv preprint arXiv:1807.04689},
  year      = 2018,
  html      = {https://arxiv.org/abs/1807.04689},
  pdf       = {https://arxiv.org/pdf/1807.04689.pdf},
  abbr      = {{arXiv}},
  abstract  = {The manifold hypothesis states that many kinds of high-dimensional data are concentrated near a low-dimensional manifold. If the topology of this data manifold is non-trivial, a continuous encoder network cannot embed it in a one-to-one manner without creating holes of low density in the latent space. This is at odds with the Gaussian prior assumption typically made in Variational Auto-Encoders (VAEs), because the density of a Gaussian concentrates near a blob-like manifold. In this paper we investigate the use of manifold-valued latent variables. Specifically, we focus on the important case of continuously differentiable symmetry groups (Lie groups), such as the group of 3D rotations SO(3). We show how a VAE with SO(3)-valued latent variables can be constructed, by extending the reparameterization trick to compact connected Lie groups. Our experiments show that choosing manifold-valued latent variables that match the topology of the latent data manifold, is crucial to preserve the topological structure and learn a well-behaved latent space.},
}


@InProceedings{Weiler2019E2CNN,
  title     = {General {E}(2)-Equivariant Steerable {CNN}s},
  author    = {Weiler, Maurice and Cesa, Gabriele},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year      = 2019,
  html      = {https://arxiv.org/abs/1911.08251},
  pdf       = {https://arxiv.org/pdf/1911.08251.pdf},
  abbr      = {{NeurIPS}},
  abstract  = {The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of E(2)-equivariant convolutions in the framework of Steerable CNNs. The theory of Steerable CNNs thereby yields constraints on the convolution kernels which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group E(2) and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. E(2)-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as drop in replacement for non-equivariant convolutions.},
}


@InProceedings{Cheng2019covariance,
  title     = {Covariance in Physics and Convolutional Neural Networks},
  author    = {Cheng, Miranda and Anagiannis, Vassilis and Weiler, Maurice and de Haan, Pim and Cohen, Taco and Welling, Max},
  booktitle = {ICML 2019 Workshop on Theoretical Physics for Deep Learning},
  year      = 2019,
  html      = {https://arxiv.org/abs/1906.02481},
  pdf       = {https://arxiv.org/pdf/1906.02481.pdf},
  abstract  = {In this proceeding we give an overview of the idea of covariance (or equivariance) featured in the recent development of convolutional neural networks (CNNs). We study the similarities and differences between the use of covariance in theoretical physics and in the CNN context. Additionally, we demonstrate that the simple assumption of covariance, together with the required properties of locality, linearity and weight sharing, is sufficient to uniquely determine the form of the convolution.},
}


@InProceedings{Cohen2019GeneralTheory,
  title     = {A General Theory of Equivariant {CNN}s on Homogeneous Spaces},
  author    = {Cohen, Taco S. and Geiger, Mario and Weiler, Maurice},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year      = 2019,
  html      = {https://arxiv.org/abs/1811.02017},
  pdf       = {https://arxiv.org/pdf/1811.02017.pdf},
  abbr      = {{NeurIPS}},
  abstract  = {We present a general theory of Group equivariant Convolutional Neural Networks (G-CNNs) on homogeneous spaces such as Euclidean space and the sphere. Feature maps in these networks represent fields on a homogeneous base space, and layers are equivariant maps between spaces of fields. The theory enables a systematic classification of all existing G-CNNs in terms of their symmetry group, base space, and field type. We also consider a fundamental question: what is the most general kind of equivariant linear map between feature spaces (fields) of given types? Following Mackey, we show that such maps correspond one-to-one with convolutions using equivariant kernels, and characterize the space of such kernels.},
}


@InProceedings{Cohen2019Gauge,
  title     = {Gauge Equivariant Convolutional Networks and the {Icosahedral CNN}},
  author    = {Cohen, Taco S. and Weiler, Maurice and Kicanaoglu, Berkay and Welling, Max},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = 2019,
  html      = {https://arxiv.org/abs/1902.04615},
  pdf       = {https://arxiv.org/pdf/1902.04615.pdf},
  abbr      = {{ICML}},
  abstract  = {The principle of equivariance to symmetry transformations enables a theoretically grounded approach to neural network architecture design. Equivariant networks have shown excellent performance and data efficiency on vision and medical imaging problems that exhibit symmetries. Here we show how this principle can be extended beyond global symmetries to local gauge transformations. This enables the development of a very general class of convolutional neural networks on manifolds that depend only on the intrinsic geometry, and which includes many popular methods from equivariant and geometric deep learning. We implement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which provides a reasonable approximation of the sphere. By choosing to work with this very regular manifold, we are able to implement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and practical alternative to Spherical CNNs. Using this method, we demonstrate substantial improvements over previous methods on the task of segmenting omnidirectional images and global climate patterns.},
}


@InProceedings{deHaan2020meshCNNs,
  title     = {Gauge Equivariant Mesh {CNN}s: Anisotropic convolutions on geometric graphs},
  author    = {de Haan, Pim and Weiler, Maurice and Cohen, Taco and Welling, Max},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = 2021,
  html      = {https://arxiv.org/abs/2003.05425},
  pdf       = {https://arxiv.org/pdf/2003.05425.pdf},
  abbr      = {{ICLR}},
  abstract  = {A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs). Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.},
}


@InProceedings{Lang2020WignerEckart,
  title     = {{A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels}},
  author    = {Lang, Leon and Weiler, Maurice},
  booktitle = {{International Conference on Learning Representations (ICLR)}},
  year      = 2020,
  html      = {https://arxiv.org/abs/2010.10952},
  pdf       = {https://arxiv.org/pdf/2010.10952.pdf},
  abbr      = {{ICLR}},
  abstract  = {Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with G-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the G-steerability constraint has been derived, it has to date only been solved for specific use cases - a general characterization of G-steerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of G being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, and 3) harmonic basis functions on homogeneous spaces.},
}


@InProceedings{Weiler2021CoordinateIndependent,
  title     = {{Coordinate Independent Convolutional Networks - Isometry and Gauge Equivariant Convolutions on Riemannian Manifolds}},
  author    = {Weiler, Maurice and Forr{\'e}, Patrick and Verlinde, Erik and Welling, Max},
  booktitle = {{International Conference on Learning Representations (ICLR)}},
  year      = 2020,
  html      = {https://arxiv.org/abs/2106.06020},
  pdf       = {https://arxiv.org/pdf/2106.06020.pdf},
  abbr      = {{arXiv}},
  abstract  = {Motivated by the vast success of deep convolutional networks, there is a great interest in generalizing convolutions to non-Euclidean manifolds. A major complication in comparison to flat spaces is that it is unclear in which alignment a convolution kernel should be applied on a manifold. The underlying reason for this ambiguity is that general manifolds do not come with a canonical choice of reference frames (gauge). Kernels and features therefore have to be expressed relative to arbitrary coordinates. We argue that the particular choice of coordinatization should not affect a network's inference - it should be coordinate independent. A simultaneous demand for coordinate independence and weight sharing is shown to result in a requirement on the network to be equivariant under local gauge transformations (changes of local reference frames). The ambiguity of reference frames depends thereby on the G-structure of the manifold, such that the necessary level of gauge equivariance is prescribed by the corresponding structure group G. Coordinate independent convolutions are proven to be equivariant w.r.t. those isometries that are symmetries of the G-structure. The resulting theory is formulated in a coordinate free fashion in terms of fiber bundles. To exemplify the design of coordinate independent convolutions, we implement a convolutional network on the M\"obius strip. The generality of our differential geometric formulation of convolutional networks is demonstrated by an extensive literature review which explains a large number of Euclidean CNNs, spherical CNNs and CNNs on general surfaces as specific instances of coordinate independent convolutions.},
}


@InProceedings{Jenner2021steerablePDO,
 title     = {{Steerable Partial Differential Operators for Equivariant Neural Networks}},
 author    = {Jenner, Erik and Weiler, Maurice},
 booktitle = "{International Conference on Learning Representations (ICLR)}",
 year      = 2022,
 html      = {https://arxiv.org/abs/2106.10163},
 pdf       = {https://arxiv.org/pdf/2106.10163.pdf},
 abbr      = {{ICLR}},
 abstract  = {Recent work in equivariant deep learning bears strong similarities to physics. Fields over a base space are fundamental entities in both subjects, as are equivariant maps between these fields. In deep learning, however, these maps are usually defined by convolutions with a kernel, whereas they are partial differential operators (PDOs) in physics. Developing the theory of equivariant PDOs in the context of deep learning could bring these subjects even closer together and lead to a stronger flow of ideas. In this work, we derive a G-steerability constraint that completely characterizes when a PDO between feature vector fields is equivariant, for arbitrary symmetry groups G. We then fully solve this constraint for several important groups. We use our solutions as equivariant drop-in replacements for convolutional layers and benchmark them in that role. Finally, we develop a framework for equivariant maps based on Schwartz distributions that unifies classical convolutions and differential operators and gives insight about the relation between the two.},
}


@InProceedings{Cesa2021ENsteerable,
 title     = {{A program to build E(N)-equivariant steerable CNNs}},
 author    = {Cesa, Gabriele and Lang, Leon and Weiler, Maurice},
 booktitle = "{International Conference on Learning Representations (ICLR)}",
 year      = 2022,
 html      = {https://openreview.net/forum?id=WE4qe9xlnQw&noteId=5p20fBDe_z4},
 pdf       = {https://openreview.net/pdf?id=WE4qe9xlnQw},
 abbr      = {{ICLR}},
 abstract  = {Equivariance is becoming an increasingly popular design choice to build data efficient neural networks by exploiting prior knowledge about the symmetries of the problem at hand. Euclidean steerable CNNs are one of the most common classes of equivariant networks. While the constraints these architectures need to satisfy are understood, no practical method to parametrize them generally has been described so far, with most existing approaches tailored to specific groups or classes of groups. In this work, we generalize the Wigner-Eckart theorem proposed in (Lang et al.), which characterizes general G-steerable kernel spaces for compact groups G over their homogeneous spaces, to arbitrary G-spaces. This enables us to directly parameterize filters in terms of a band-limited basis on the base space, but also to easily implement steerable CNNs equivariant to a large number of groups. To demonstrate its generality, we instantiate our method on a large variety of isometry groups acting on the Euclidean space R^3. Our general framework allows us to build E(3) and SE(3)-steerable CNNs like previous works, but also CNNs with arbitrary G<=O(3)-steerable kernels. For example, we build 3D CNNs equivariant to the symmetries of platonic solids or choose G=SO(2) when working with 3D data having only azimuthal symmetries. We compare these models on 3D shapes and molecular datasets, observing improved performance by matching the model's symmetries to the ones of the data.},
}
